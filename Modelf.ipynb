{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1ddLMo0fezT",
    "outputId": "29e3ba32-1e2e-4776-f65e-290d0177d9b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Kaggle Migration: Mounted drive logic removed.\n",
    "# If you are running on Kaggle, your data will be in /kaggle/input/\n",
    "# and your outputs will be in /kaggle/working/\n",
    "import os\n",
    "print(\"Running on Kaggle Environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwUjIv9Nfgru"
   },
   "outputs": [],
   "source": [
    "!pip -q install transformers torchaudio librosa soundfile tqdm scikit-learn webrtcvad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CUkidnKgA5Y"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "from transformers import WavLMModel\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8VPFq-knLz8",
    "outputId": "5c0d3d13-f7c0-4e23-e746-026e3474d48a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# Data paths (Kaggle standard paths) - DIRECT PATH ENTRY\n",
    "# Paste your folder paths directly into these lists. \n",
    "# The code will handle flat folders, nested languages, and mixed sources automatically.\n",
    "BASE_DATA = {\n",
    "    \"Human\": [\n",
    "        \"/kaggle/input/ai-4-bharat/AI_4_bharat/Human/English\",\n",
    "        \"/kaggle/input/ai-4-bharat/AI_4_bharat/Human/Hindi\",\n",
    "        \"/kaggle/input/ai-4-bharat/AI_4_bharat/Human/Tamil\",\n",
    "        \"/kaggle/input/ai-4-bharat/AI_4_bharat/Human/Telugu\",\n",
    "        \"/kaggle/input/ai-4-bharat/AI_4_bharat/Human/Malayalam\"\n",
    "    ],\n",
    "    \"AI\": [\n",
    "        \"/kaggle/input/datasets/harshshah9104/ai-summit/AI/English-20260213T140035Z-3-001/English\",\n",
    "        \"/kaggle/input/datasets/harshshah9104/ai-summit/AI/Hindi-20260213T140140Z-3-001/Hindi\",\n",
    "        \"/kaggle/input/datasets/harshshah9104/ai-summit/AI/Tamil-20260213T135756Z-3-001/Tamil\",\n",
    "        \"/kaggle/input/datasets/harshshah9104/ai-summit/AI/Telugu-20260213T135744Z-3-001/Telugu\",\n",
    "        \"/kaggle/input/datasets/harshshah9104/ai-summit/AI/Malyalam-20260213T135808Z-3-001/Malyalam\"\n",
    "    ]\n",
    "}\n",
    "SAVE_DIR = \"/kaggle/working/wavlm_ensemble_checkpoints\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Audio processing - FIXED LENGTH\n",
    "SAMPLE_RATE = 16000\n",
    "TARGET_DURATION = 5.0  # All audio will be exactly 5 seconds\n",
    "# This means: 5.0 * 16000 = 80,000 samples per audio\n",
    "\n",
    "# Allowed audio formats in dataset folders (Lowercased for comparison)\n",
    "AUDIO_EXTS = (\".wav\", \".mp3\", \".flac\")\n",
    "\n",
    "# Normalization settings\n",
    "NORM_TYPE = \"peak\"           # \"peak\" or \"rms\"\n",
    "RMS_TARGET = 0.1             # Target RMS level for RMS normalization\n",
    "SILENCE_THRESHOLD = 1e-4     # Threshold to detect silence\n",
    "\n",
    "# Preprocessing (denoise / filtering)\n",
    "USE_DENOISE = False          # Spectral-gate denoise (OFF per ASVspoof)\n",
    "DENOISE_N_FFT = 1024\n",
    "DENOISE_HOP_LENGTH = 256\n",
    "DENOISE_NOISE_PERCENTILE = 10\n",
    "DENOISE_THRESHOLD_MULT = 1.5\n",
    "DENOISE_ATTENUATION = 0.2\n",
    "\n",
    "USE_BANDPASS = True\n",
    "HIGHPASS_CUTOFF_HZ = 80.0\n",
    "LOWPASS_CUTOFF_HZ = 7800.0      # keep below Nyquist (8000 for 16kHz)\n",
    "\n",
    "# Inference-time speech selection (recommended for call recordings)\n",
    "USE_VAD_INFERENCE = True\n",
    "VAD_AGGRESSIVENESS = 2       # 0..3 (higher = more strict)\n",
    "MIN_VOICED_SECONDS = 1.0     # require at least this much speech after VAD\n",
    "MAX_INFER_WINDOWS = 6        # cap compute on long calls\n",
    "\n",
    "# Regularization / anti-overfitting\n",
    "DROPOUT_P = 0.3\n",
    "WEIGHT_DECAY = 1e-2\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "MIN_DELTA_AUC = 1e-4\n",
    "LABEL_SMOOTHING = 0.05           \n",
    "SPEC_AUG_FREQ_MASKS = 2          \n",
    "SPEC_AUG_FREQ_WIDTH = 30         \n",
    "SPEC_AUG_TIME_MASKS = 2          # New: Temporal robustness\n",
    "SPEC_AUG_TIME_WIDTH = 40         \n",
    "\n",
    "# Forensic Augmentation Settings\n",
    "USE_AUGMENT = True\n",
    "SPEED_PERTURB_RANGE = (0.9, 1.1)  \n",
    "GAIN_RANGE = (0.5, 1.2)           \n",
    "NOISE_STD = 0.002                 \n",
    "USE_CODEC_AUG = True              \n",
    "USE_RANDOM_EQ = True             # New: Mic/Hardware simulation\n",
    "CLIPPING_PROB = 0.2              # New: Digital artifact simulation\n",
    "\n",
    "# Model Architecture strategy\n",
    "UNFREEZE_TOP_LAYERS = 2           # New: Fine-tune last 2 layers of WavLM\n",
    "\n",
    "# Forensic Data Augmentation paths (Kaggle)\n",
    "ESC50_PATH = \"/kaggle/input/esc50/ESC-50-master/audio\" # Adjust to your ESC-50 Kaggle path\n",
    "HUMAN_NOISE_COUNT = 50            \n",
    "MP3_BITRATE = \"32k\"               \n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10  # Kaggle GPUs are faster, can do more epochs\n",
    "LEARNING_RATE = 2e-4   \n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "# Ensemble weights\n",
    "AASIST_WEIGHT = 0.6\n",
    "OCSOFT_WEIGHT = 0.4\n",
    "\n",
    "# Cache settings (only for validation)\n",
    "MAX_CACHE_SIZE = 1000\n",
    "\n",
    "# Class-imbalance handling (instead of oversampling): BCE pos_weight for AI=1\n",
    "\n",
    "OVERSAMPLE_AI = True  # oversample minority class via WeightedRandomSampler\n",
    "POS_WEIGHT_AI = None  # set automatically after train/val split\n",
    "# Class-imbalance handling (instead of oversampling): BCE pos_weight for AI=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1P_yRlTKnTt3",
    "outputId": "c8a42968-ce90-4932-c078-83de60898cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DATA: /content/drive/MyDrive/AI_4_bharat\n",
      "Exists: True\n",
      "AUDIO_EXTS: ('.wav',)\n",
      "\n",
      "Human: 5 language folders\n",
      "  English: 2500 wav files (sample: ['real_1064.wav', 'real_1041.wav', 'real_1031.wav', 'real_1054.wav', 'real_1053.wav'])\n",
      "  Hindi: 2660 wav files (sample: ['Hindi_002229.wav', 'Hindi_002230.wav', 'Hindi_002231.wav', 'Hindi_002232.wav', 'Hindi_002233.wav'])\n",
      "  Malayalam: 2710 wav files (sample: ['Malayalam_001002.wav', 'Malayalam_001003.wav', 'Malayalam_001004.wav', 'Malayalam_001005.wav', 'Malayalam_001001.wav'])\n",
      "  Tamil: 2870 wav files (sample: ['Tamil_001287.wav', 'Tamil_001289.wav', 'Tamil_001290.wav', 'Tamil_001291.wav', 'Tamil_002583.wav'])\n",
      "  Telugu: 2560 wav files (sample: ['Telugu_002512.wav', 'Telugu_002513.wav', 'Telugu_001049.wav', 'Telugu_002514.wav', 'Telugu_001050.wav'])\n",
      "\n",
      "AI: 5 language folders\n",
      "  English: 2500 wav files (sample: ['synthetic_1500.wav', 'synthetic_1501.wav', 'synthetic_1502.wav', 'synthetic_1503.wav', 'synthetic_1504.wav'])\n",
      "  Hindi: 1749 wav files (sample: ['Hindi_AI_000750.wav', 'Hindi_AI_000751.wav', 'Hindi_AI_000752.wav', 'Hindi_AI_000753.wav', 'Hindi_AI_000754.wav'])\n",
      "  Malayalam: 1777 wav files (sample: ['Malayalam_AI_000778.wav', 'Malayalam_AI_000779.wav', 'Malayalam_AI_000780.wav', 'Malayalam_AI_000781.wav', 'Malayalam_AI_000782.wav'])\n",
      "  Tamil: 1919 wav files (sample: ['Tamil_AI_000920.wav', 'Tamil_AI_000921.wav', 'Tamil_AI_000922.wav', 'Tamil_AI_000923.wav', 'Tamil_AI_000924.wav'])\n",
      "  Telugu: 1716 wav files (sample: ['Telugu_AI_000717.wav', 'Telugu_AI_000718.wav', 'Telugu_AI_000719.wav', 'Telugu_AI_000720.wav', 'Telugu_AI_000721.wav'])\n",
      "\n",
      "Total WAV files seen across dataset: 22961\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check: do we actually have WAVs where we think we do?\n",
    "import os\n",
    "\n",
    "print(\"BASE_DATA Configuration:\", BASE_DATA)\n",
    "# Fixed: handled dict type for the existence check\n",
    "if isinstance(BASE_DATA, str):\n",
    "    print(\"Base Path Exists:\", os.path.isdir(BASE_DATA))\n",
    "else:\n",
    "    print(\"Mode: Multiple Dataset Paths\")\n",
    "    \n",
    "print(\"AUDIO_EXTS:\", AUDIO_EXTS)\n",
    "\n",
    "def _count_wavs(folder: str):\n",
    "    try:\n",
    "        files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    except Exception:\n",
    "        return 0, []\n",
    "    # Case-insensitive extension check\n",
    "    wavs = [f for f in files if f.lower().endswith(AUDIO_EXTS)]\n",
    "    return len(wavs), wavs[:5]\n",
    "\n",
    "total_wavs = 0\n",
    "for cls in [\"Human\", \"AI\"]:\n",
    "    if isinstance(BASE_DATA, dict):\n",
    "        cls_paths = BASE_DATA.get(cls, [])\n",
    "        if isinstance(cls_paths, str): cls_paths = [cls_paths]\n",
    "    else:\n",
    "        cls_paths = [os.path.join(BASE_DATA, cls)]\n",
    "        \n",
    "    for cls_path in cls_paths:\n",
    "        if not os.path.isdir(cls_path):\n",
    "            print(f\"⚠️ Missing folder: {cls_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Detect if it's flat or nested with languages\n",
    "        items = sorted(os.listdir(cls_path))\n",
    "        langs = [d for d in items if os.path.isdir(os.path.join(cls_path, d))]\n",
    "        \n",
    "        if langs:\n",
    "            print(f\"\\n{cls} (dataset: {os.path.basename(cls_path)}): {len(langs)} language folders\")\n",
    "            for lang in langs:\n",
    "                lang_path = os.path.join(cls_path, lang)\n",
    "                n, sample = _count_wavs(lang_path)\n",
    "                total_wavs += n\n",
    "                print(f\"  {lang}: {n} wav files (sample: {sample})\")\n",
    "        else:\n",
    "            n, sample = _count_wavs(cls_path)\n",
    "            total_wavs += n\n",
    "            print(f\"\\n{cls} (Flat dataset: {os.path.basename(cls_path)}): {n} wav files\")\n",
    "\n",
    "print(f\"\\nTotal WAV files seen across dataset: {total_wavs}\")\n",
    "if total_wavs == 0:\n",
    "    print(\"\\nIf this is 0, then either:\")\n",
    "    print(\"- your Drive path is different than BASE_DATA, or\")\n",
    "    print(\"- your files aren’t actually .wav in those folders, or\")\n",
    "    print(\"- they’re nested deeper (this loader only scans one level), or\")\n",
    "    print(\"- the conversion cell didn’t run / wrote WAVs elsewhere.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92bOoVvenVcT"
   },
   "outputs": [],
   "source": [
    "def _apply_bandpass_torch(wav_t: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "    \"\"\"Bandpass filter to focus on speech band and reduce rumble/hiss.\"\"\"\n",
    "    if not USE_BANDPASS:\n",
    "        return wav_t\n",
    "    wav_t = torchaudio.functional.highpass_biquad(wav_t, sr, cutoff_freq=HIGHPASS_CUTOFF_HZ)\n",
    "    wav_t = torchaudio.functional.lowpass_biquad(wav_t, sr, cutoff_freq=LOWPASS_CUTOFF_HZ)\n",
    "    return wav_t\n",
    "\n",
    "\n",
    "def _denoise_spectral_gate_np(wav_np: np.ndarray, _sr: int) -> np.ndarray:\n",
    "    \"\"\"Mild spectral gating denoise (keeps speech; reduces steady background noise).\"\"\"\n",
    "    if not USE_DENOISE:\n",
    "        return wav_np\n",
    "    if wav_np.size == 0:\n",
    "        return wav_np\n",
    "    if not np.isfinite(wav_np).all():\n",
    "        return wav_np\n",
    "\n",
    "    stft = librosa.stft(wav_np, n_fft=DENOISE_N_FFT, hop_length=DENOISE_HOP_LENGTH)\n",
    "    mag = np.abs(stft)\n",
    "    phase = np.exp(1j * np.angle(stft))\n",
    "\n",
    "    noise_floor = np.percentile(mag, DENOISE_NOISE_PERCENTILE, axis=1, keepdims=True)\n",
    "    thresh = noise_floor * float(DENOISE_THRESHOLD_MULT)\n",
    "\n",
    "    mask = (mag >= thresh).astype(np.float32)\n",
    "    mag_d = mag * mask + mag * (1.0 - mask) * float(DENOISE_ATTENUATION)\n",
    "\n",
    "    stft_d = mag_d * phase\n",
    "    wav_out = librosa.istft(stft_d, hop_length=DENOISE_HOP_LENGTH, length=len(wav_np))\n",
    "    return wav_out.astype(np.float32)\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    \"\"\"Dataset with forensic augmentation: Human-noise injection + ESC-50 + MP3.\"\"\"\n",
    "\n",
    "    def __init__(self, base_dir, target_duration=5.0, mode='train', max_cache_size=1000):\n",
    "        self.samples = []\n",
    "        self.cache = OrderedDict()\n",
    "        self.max_cache_size = max_cache_size\n",
    "        self.target_duration = target_duration\n",
    "        self.mode = mode\n",
    "        self.failed_files = []\n",
    "        \n",
    "        # Forensic Noise Banks\n",
    "        self.human_noises = []\n",
    "        self.esc50_bank = []\n",
    "\n",
    "        # 1. Collect all audio files (Human/AI)\n",
    "        human_files = []\n",
    "        for label, cls in [(0, \"Human\"), (1, \"AI\")]:\n",
    "            # Use specific path list from dict if available\n",
    "            if isinstance(base_dir, dict):\n",
    "                cls_paths = base_dir.get(cls, [])\n",
    "                if isinstance(cls_paths, str): cls_paths = [cls_paths]\n",
    "            else:\n",
    "                cls_paths = [os.path.join(base_dir, cls)]\n",
    "                \n",
    "            for cls_path in cls_paths:\n",
    "                if not os.path.isdir(cls_path): \n",
    "                    print(f\"⚠️ Warning: Folder not found: {cls_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Check if this specific folder has subfolders (nested language structure)\n",
    "                # or contains files directly (direct language structure)\n",
    "                items = sorted(os.listdir(cls_path))\n",
    "                subdirs = [d for d in items if os.path.isdir(os.path.join(cls_path, d))]\n",
    "                \n",
    "                if subdirs:\n",
    "                    # Recursive collection for nested folders\n",
    "                    for lang in subdirs:\n",
    "                        lang_path = os.path.join(cls_path, lang)\n",
    "                        found_in_subdir = 0\n",
    "                        for f in os.listdir(lang_path):\n",
    "                            if f.lower().endswith(AUDIO_EXTS):\n",
    "                                path = os.path.join(lang_path, f)\n",
    "                                self.samples.append((path, label, lang))\n",
    "                                if label == 0: human_files.append(path)\n",
    "                                found_in_subdir += 1\n",
    "                        if found_in_subdir > 0:\n",
    "                            print(f\"  - Loaded {found_in_subdir} files from {lang}\")\n",
    "                else:\n",
    "                    # Direct collection from the provided folder\n",
    "                    folder_name = os.path.basename(cls_path)\n",
    "                    found_direct = 0\n",
    "                    for f in items:\n",
    "                        if f.lower().endswith(AUDIO_EXTS):\n",
    "                            path = os.path.join(cls_path, f)\n",
    "                            self.samples.append((path, label, folder_name))\n",
    "                            if label == 0: human_files.append(path)\n",
    "                            found_direct += 1\n",
    "                    if found_direct > 0:\n",
    "                        print(f\"  - Loaded {found_direct} files from {folder_name}\")\n",
    "                    else:\n",
    "                        print(f\"  ⚠️ No valid audio files found in: {cls_path}\")\n",
    "\n",
    "        # 2. Extract background noise from Human dataset (only for training)\n",
    "        if mode == 'train':\n",
    "            print(f\"Extracting background noise from first {HUMAN_NOISE_COUNT} human files...\")\n",
    "            for p in human_files[:HUMAN_NOISE_COUNT]:\n",
    "                noise_segment = self._extract_background_noise(p)\n",
    "                if noise_segment is not None:\n",
    "                    self.human_noises.append(noise_segment)\n",
    "            \n",
    "            # 3. Pre-load a small bank of ESC-50 noises (Avoids disk I/O in loop)\n",
    "            if os.path.isdir(ESC50_PATH):\n",
    "                all_esc = []\n",
    "                for root, _, files in os.walk(ESC50_PATH):\n",
    "                    for f in files:\n",
    "                        if f.lower().endswith(\".wav\"):\n",
    "                            all_esc.append(os.path.join(root, f))\n",
    "                \n",
    "                print(f\"Pre-loading bank of 100 ESC-50 noises...\")\n",
    "                for p in all_esc[:100]:\n",
    "                    try:\n",
    "                        n, _ = librosa.load(p, sr=SAMPLE_RATE, duration=5.0, mono=True)\n",
    "                        self.esc50_bank.append(torch.tensor(n).float())\n",
    "                    except: continue\n",
    "                print(f\"Bank ready with {len(self.esc50_bank)} noise profiles.\")\n",
    "\n",
    "    def _extract_background_noise(self, path):\n",
    "        \"\"\"Finds 'silent' or low-energy regions in a human file to use as noise.\"\"\"\n",
    "        try:\n",
    "            # Load short snippet to save memory\n",
    "            wav, _ = librosa.load(path, sr=SAMPLE_RATE, duration=10, mono=True)\n",
    "            if len(wav) < SAMPLE_RATE: return None\n",
    "            \n",
    "            # Find low energy segments\n",
    "            intervals = librosa.effects.split(wav, top_db=30) # db threshold for \"silence\"\n",
    "            # Invert intervals to find the \"silence\" parts\n",
    "            noise_segs = []\n",
    "            last_end = 0\n",
    "            for start, end in intervals:\n",
    "                if start > last_end + int(0.5 * SAMPLE_RATE):\n",
    "                    noise_segs.append(wav[last_end:start])\n",
    "                last_end = end\n",
    "            \n",
    "            if not noise_segs:\n",
    "                # If no clear silence, just use the whole file at low amplitude\n",
    "                return torch.tensor(wav).float()\n",
    "            \n",
    "            # Pick longest silence\n",
    "            noise_segs.sort(key=len, reverse=True)\n",
    "            return torch.tensor(noise_segs[0]).float()\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _normalize_audio(self, wav):\n",
    "        \"\"\"Robust audio normalization.\"\"\"\n",
    "        if np.abs(wav).max() < SILENCE_THRESHOLD:\n",
    "            return wav\n",
    "\n",
    "        if NORM_TYPE == \"peak\":\n",
    "            wav = wav / max(np.abs(wav).max(), 1e-6)\n",
    "        elif NORM_TYPE == \"rms\":\n",
    "            rms = np.sqrt(np.mean(wav**2))\n",
    "            if rms > 1e-6:\n",
    "                wav = wav * (RMS_TARGET / rms)\n",
    "                wav = np.clip(wav, -1.0, 1.0)\n",
    "\n",
    "        return wav\n",
    "\n",
    "    def _load_full_audio(self, path):\n",
    "        \"\"\"Load, denoise/filter, and normalize FULL audio (cache this).\"\"\"\n",
    "        try:\n",
    "            wav, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
    "\n",
    "            if len(wav) == 0:\n",
    "                raise ValueError(\"Empty audio file\")\n",
    "            if not np.isfinite(wav).all():\n",
    "                raise ValueError(\"Audio contains NaN or Inf values\")\n",
    "\n",
    "            # Simple bandpass\n",
    "            wav_t = torch.tensor(wav).float()\n",
    "            wav_t = _apply_bandpass_torch(wav_t, SAMPLE_RATE)\n",
    "            wav = wav_t.cpu().numpy()\n",
    "\n",
    "            # Normalize full audio\n",
    "            wav = self._normalize_audio(wav)\n",
    "\n",
    "            return torch.tensor(wav).float()\n",
    "\n",
    "        except Exception as e:\n",
    "            self.failed_files.append((path, str(e)))\n",
    "            return None\n",
    "\n",
    "    def _crop_to_fixed_duration(self, wav):\n",
    "        \"\"\"Crop cached audio to fixed duration.\"\"\"\n",
    "        target_length = int(self.target_duration * SAMPLE_RATE)\n",
    "        current_length = len(wav)\n",
    "\n",
    "        if current_length == 0:\n",
    "            return torch.zeros(target_length).float()\n",
    "\n",
    "        if current_length < target_length:\n",
    "            # Zero-pad short clips\n",
    "            pad_length = target_length - current_length\n",
    "            wav = torch.cat([wav, torch.zeros(pad_length)])\n",
    "            return wav\n",
    "\n",
    "        if current_length > target_length:\n",
    "            if self.mode == 'train':\n",
    "                max_start = current_length - target_length\n",
    "                start = np.random.randint(0, max_start + 1)\n",
    "            else:\n",
    "                start = (current_length - target_length) // 2\n",
    "            wav = wav[start:start + target_length]\n",
    "\n",
    "        return wav\n",
    "\n",
    "    def _apply_codec_sim(self, wav: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Simulate MP3 compression and telephony codecs.\"\"\"\n",
    "        if not USE_CODEC_AUG: return wav\n",
    "        \n",
    "        # Randomly choose between real MP3 simulation or simple resampling\n",
    "        if torch.rand(1).item() < 0.5:\n",
    "            # Fallback: Resampling + Bit-depth reduction (avoids ffmpeg/sox errors)\n",
    "            low_sr = torch.randint(8000, 12000, (1,)).item()\n",
    "            wav = torchaudio.functional.resample(wav.unsqueeze(0), SAMPLE_RATE, low_sr)\n",
    "            wav = torchaudio.functional.resample(wav, low_sr, SAMPLE_RATE).squeeze(0)\n",
    "            bits = torch.randint(4, 9, (1,)).item()\n",
    "            wav = torch.round(wav * (2**(bits-1))) / (2**(bits-1))\n",
    "        return wav\n",
    "\n",
    "    def _apply_random_eq(self, wav: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Simulate different microphone quality by random EQ boosts/cuts.\"\"\"\n",
    "        if not USE_RANDOM_EQ: return wav\n",
    "        try:\n",
    "            # Randomly boost/cut lows or highs\n",
    "            f0 = torch.randint(100, 3000, (1,)).item()\n",
    "            gain = torch.empty(1).uniform_(-6, 6).item() # +/- 6dB\n",
    "            wav = torchaudio.functional.lowpass_biquad(wav.unsqueeze(0), SAMPLE_RATE, f0).squeeze(0) if gain < 0 else wav\n",
    "            \n",
    "            # Random peak filter\n",
    "            f_center = torch.randint(500, 4000, (1,)).item()\n",
    "            gain_p = torch.empty(1).uniform_(-10, 5).item()\n",
    "            wav = torchaudio.functional.equalizer_biquad(wav.unsqueeze(0), SAMPLE_RATE, f_center, gain_p, Q=0.707).squeeze(0)\n",
    "        except: pass\n",
    "        return wav\n",
    "\n",
    "    def _generate_pink_noise(self, length: int) -> torch.Tensor:\n",
    "        \"\"\"Generates pink noise by filtering white noise (approx -3dB/octave).\"\"\"\n",
    "        white = torch.randn(length)\n",
    "        b = [0.049922035, -0.095993537, 0.050223151, -0.004947755, 0.000000000, 0.000000000, 0.000000000]\n",
    "        fft = torch.fft.rfft(white)\n",
    "        f = torch.linspace(1, len(fft), len(fft))\n",
    "        fft = fft / torch.sqrt(f) \n",
    "        return torch.fft.irfft(fft, n=length)\n",
    "\n",
    "    def _augment(self, wav: torch.Tensor, label: int) -> torch.Tensor:\n",
    "        \"\"\"Enhanced Forensic suite with Hierarchical Noise Probabilities.\"\"\"\n",
    "        if not USE_AUGMENT:\n",
    "            return wav\n",
    "\n",
    "        target_length = int(self.target_duration * SAMPLE_RATE)\n",
    "\n",
    "        # 1. Hardware & Gain (Independent)\n",
    "        wav = wav * float(torch.empty(1).uniform_(*GAIN_RANGE))\n",
    "        wav = self._apply_random_eq(wav)\n",
    "\n",
    "        # 2. Hierarchical Noise Selection (User Specified)\n",
    "        noise_roll = torch.rand(1).item()\n",
    "        \n",
    "        # Mode 1: Untouched (10%) - No noise stage\n",
    "        if noise_roll < 0.10:\n",
    "            pass \n",
    "            \n",
    "        # Mode 2: Pink/White Noise (20%)\n",
    "        elif noise_roll < 0.30:\n",
    "            snr = torch.randint(20, 35, (1,)).item()\n",
    "            alpha = 10**(-snr/20)\n",
    "            if torch.rand(1).item() < 0.7:\n",
    "                noise = self._generate_pink_noise(len(wav))\n",
    "            else:\n",
    "                noise = torch.randn_like(wav)\n",
    "            wav = torch.clamp(wav + alpha * noise, -1.0, 1.0)\n",
    "\n",
    "        # Mode 3: Human Room-Hum Injection (35%)\n",
    "        elif noise_roll < 0.65:\n",
    "            # Primarily for AI. If human, this effectively stays clean/untouched room hum.\n",
    "            if label == 1 and self.human_noises:\n",
    "                 noise = self.human_noises[np.random.randint(len(self.human_noises))]\n",
    "                 snr = torch.randint(15, 30, (1,)).item()\n",
    "                 alpha = 10**(-snr/20)\n",
    "                 if len(noise) < len(wav):\n",
    "                     noise = noise.repeat(int(np.ceil(len(wav)/len(noise))))[:len(wav)]\n",
    "                 else:\n",
    "                     noise = noise[:len(wav)]\n",
    "                 wav = torch.clamp(wav + alpha * noise, -1.0, 1.0)\n",
    "\n",
    "        # Mode 4: ESC-50 Environmental (35%)\n",
    "        else:\n",
    "            if self.esc50_bank:\n",
    "                noise = self.esc50_bank[np.random.randint(len(self.esc50_bank))]\n",
    "                alpha = torch.empty(1).uniform_(0.01, 0.04).item()\n",
    "                if len(noise) < len(wav):\n",
    "                    noise = noise.repeat(int(np.ceil(len(wav)/len(noise))))[:len(wav)]\n",
    "                else:\n",
    "                    noise = noise[:len(wav)]\n",
    "                wav = torch.clamp(wav + alpha * noise, -1.0, 1.0)\n",
    "\n",
    "        # 3. Codec & Digital Artifacts (Independent)\n",
    "        wav = self._apply_codec_sim(wav)\n",
    "        \n",
    "        if torch.rand(1).item() < CLIPPING_PROB:\n",
    "            limit = torch.empty(1).uniform_(0.7, 0.95).item()\n",
    "            wav = torch.clamp(wav, -limit, limit)\n",
    "\n",
    "        # 4. Speed perturbation (Independent)\n",
    "        if torch.rand(1).item() < 0.3:\n",
    "            speed = float(torch.empty(1).uniform_(*SPEED_PERTURB_RANGE))\n",
    "            wav = torchaudio.functional.resample(wav.unsqueeze(0), int(SAMPLE_RATE * speed), SAMPLE_RATE).squeeze(0)\n",
    "\n",
    "        # Ensure exact length\n",
    "        if len(wav) > target_length:\n",
    "            wav = wav[:target_length]\n",
    "        elif len(wav) < target_length:\n",
    "            wav = torch.cat([wav, torch.zeros(target_length - len(wav))])\n",
    "\n",
    "        return wav\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label, _lang = self.samples[idx]\n",
    "\n",
    "        if path not in self.cache:\n",
    "            if len(self.cache) >= self.max_cache_size: self.cache.popitem(last=False)\n",
    "            self.cache[path] = self._load_full_audio(path)\n",
    "        else:\n",
    "            self.cache.move_to_end(path)\n",
    "\n",
    "        full_audio = self.cache[path]\n",
    "        if full_audio is None: return None\n",
    "\n",
    "        cropped_audio = self._crop_to_fixed_duration(full_audio)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            cropped_audio = self._augment(cropped_audio, label)\n",
    "\n",
    "        return cropped_audio, label\n",
    "\n",
    "    def get_failed_files(self):\n",
    "        return self.failed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMQIXuky33LG"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Fixed-length collate with invalid-sample filtering.\"\"\"\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None, None\n",
    "\n",
    "    waves, labels = zip(*batch)\n",
    "    waves = torch.stack(waves)\n",
    "    labels = torch.tensor(labels)\n",
    "    return waves, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOy-RbU2oXXX",
    "outputId": "bc7b56e4-abb1-44a9-e5c1-2c17b07e82e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples collected: 22961\n",
      "Each audio is exactly 5.0 seconds = 80000 samples\n"
     ]
    }
   ],
   "source": [
    "# Create dataset instances for train and val modes\n",
    "train_dataset_full = AudioDataset(BASE_DATA, target_duration=TARGET_DURATION,\n",
    "                                   mode='train', max_cache_size=MAX_CACHE_SIZE)\n",
    "val_dataset_full = AudioDataset(BASE_DATA, target_duration=TARGET_DURATION,\n",
    "                                 mode='val', max_cache_size=MAX_CACHE_SIZE)\n",
    "\n",
    "print(f\"Total samples collected: {len(train_dataset_full)}\")\n",
    "print(f\"Each audio is exactly {TARGET_DURATION} seconds = {int(TARGET_DURATION * SAMPLE_RATE)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ci-1NAahoZK3",
    "outputId": "0d92fccd-90f3-4a95-b6de-8b61aa3f5f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 18368 | Human=10640, AI=7728\n",
      "Validation samples: 4593 | Human=2660, AI=1933\n",
      "\n",
      "Batch shape: (16, 80000)\n",
      "Train loader uses oversampling to balance classes.\n",
      "No padding needed - all samples exactly 5 seconds!\n"
     ]
    }
   ],
   "source": [
    "# Create stratified train/val split by (label, language)\n",
    "strata = [\n",
    "    f\"{train_dataset_full.samples[i][1]}_{train_dataset_full.samples[i][2]}\"\n",
    "    for i in range(len(train_dataset_full))\n",
    "]\n",
    "train_indices, val_indices = train_test_split(\n",
    "    range(len(train_dataset_full)),\n",
    "    test_size=VAL_SPLIT,\n",
    "    stratify=strata,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Create subset datasets\n",
    "train_dataset = Subset(train_dataset_full, train_indices)\n",
    "val_dataset = Subset(val_dataset_full, val_indices)\n",
    "\n",
    "train_subset_labels = [train_dataset_full.samples[i][1] for i in train_indices]  # 0/1\n",
    "val_subset_labels = [train_dataset_full.samples[i][1] for i in val_indices]      # 0/1\n",
    "train_counts = np.bincount(train_subset_labels, minlength=2)\n",
    "val_counts = np.bincount(val_subset_labels, minlength=2)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)} | Human={train_counts[0]}, AI={train_counts[1]}\")\n",
    "print(f\"Validation samples: {len(val_dataset)} | Human={val_counts[0]}, AI={val_counts[1]}\")\n",
    "if val_counts.min() == 0:\n",
    "    print(\"⚠️ Validation split contains only one class. AUC will be undefined (NaN).\")\n",
    "    print(\"   Fix: lower VAL_SPLIT, or stratify only by label, or ensure each (label,lang) has enough samples.\")\n",
    "\n",
    "# Class-imbalance handling: compute pos_weight ratio for reference\n",
    "POS_WEIGHT_AI = float(train_counts[0] / max(train_counts[1], 1))\n",
    "print(f\"POS_WEIGHT_AI (Human/AI): {POS_WEIGHT_AI:.3f}\")\n",
    "\n",
    "# Oversampling AI class through weighted random sampling\n",
    "# Since all AI data comes from Edge TTS, augmentation diversity is critical\n",
    "if OVERSAMPLE_AI:\n",
    "    sample_weights = []\n",
    "    for idx in train_indices:\n",
    "        label = train_dataset_full.samples[idx][1]\n",
    "        # Upweight AI samples to appear ~equally often as Human\n",
    "        weight = float(train_counts[0] / max(train_counts[1], 1)) if label == 1 else 1.0\n",
    "        sample_weights.append(weight)\n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(train_indices),\n",
    "        replacement=True,\n",
    "    )\n",
    "else:\n",
    "    train_sampler = None\n",
    "\n",
    "# Create dataloaders\n",
    "# Kaggle T4x2: num_workers=4 is ideal to balance CPU/GPU load\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=(train_sampler is None),   \n",
    "    sampler=train_sampler,             \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "if OVERSAMPLE_AI:\n",
    "    print(\"Train loader uses WeightedRandomSampler (AI oversampled via augmentation).\")\n",
    "else:\n",
    "    print(\"Train loader uses shuffle (no oversampling).\")\n",
    "\n",
    "print(f\"Batch shape: ({BATCH_SIZE}, {int(TARGET_DURATION * SAMPLE_RATE)})\")\n",
    "print(\"Regularization: label smoothing + SpecAugment + dropout.\")\n",
    "print(\"No padding needed - all samples exactly 5 seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "33997416e5a749a5a9f2318e58c72396",
      "04c91dcabde547dd8faa3759b73c74a4",
      "dc1eb4ac7fcd4990bf9ca03da42b083c",
      "ab0b2b6c45e74d088df8c3bfb8464ba4",
      "f5c9f78837974eb483017fbc01b37f65",
      "0651902164774e4abbeee973acc6e866",
      "ef50a0921ba54abfa65ef56cc40524cb",
      "cb692922692f4539990d7f4833edaf0c",
      "fbcd1f96f57b427fa91d2bab8e6c311f",
      "43c7a4523d4542f0b7c1d8073d71e03c",
      "67c47acf52094558b13c3d2582a480cb",
      "5f1478823fda4d43a38897e88a4acbc8",
      "a2a3a7cbf3ab4c8e9aa67c242f5ee554",
      "23722f8d539e470392cd349e28d04d27",
      "991d5b5f79b942cba4dd256c38b0698f",
      "bb665bf6e6854bddb66ef360a87043a9",
      "062b30d1aa4647df8d3cd9230176888f",
      "b08048eb081745eb847a45697c22fb6c",
      "9b85d9c475da4622ba9294c391172257",
      "ed265f84007f425fb71e80cee00aceb6",
      "11bc14fabe0e45728b72248b1aa57ce3",
      "a52d1cb9694547c3a72576bf755843d4"
     ]
    },
    "id": "HlRSx1tkodP8",
    "outputId": "234fe030-aecf-4a80-a91c-a7e4187de338"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33997416e5a749a5a9f2318e58c72396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1478823fda4d43a38897e88a4acbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WavLM backbone loaded and frozen.\n"
     ]
    }
   ],
   "source": [
    "# Load WavLM backbone\n",
    "wavlm = WavLMModel.from_pretrained(\"microsoft/wavlm-base\")\n",
    "\n",
    "# Multi-GPU Support for Kaggle T4x2\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    wavlm = nn.DataParallel(wavlm)\n",
    "wavlm.to(DEVICE)\n",
    "\n",
    "# Selective Unfreezing\n",
    "if UNFREEZE_TOP_LAYERS > 0:\n",
    "    # Handle DataParallel naming if active\n",
    "    model_ref = wavlm.module if hasattr(wavlm, \"module\") else wavlm\n",
    "    for name, param in model_ref.named_parameters():\n",
    "        if \"encoder.layers\" in name:\n",
    "            layer_num = int(name.split(\"encoder.layers.\")[1].split(\".\")[0])\n",
    "            if layer_num >= (12 - UNFREEZE_TOP_LAYERS):\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    print(f\"Fine-tuning top {UNFREEZE_TOP_LAYERS} layers.\")\n",
    "else:\n",
    "    for param in wavlm.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"Backbone frozen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sWnW2dI4ofPg",
    "outputId": "276ebee8-ceb0-4eb3-9939-ec1550fa31bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summary:\n",
      "WavLM parameters (frozen): 94,381,936\n",
      "Trainable parameters: 2,775,938\n",
      "Total parameters: 97,157,874\n"
     ]
    }
   ],
   "source": [
    "class AASISTHead(nn.Module):\n",
    "    \"\"\"AASIST-inspired classification head with attention + regularization.\"\"\"\n",
    "\n",
    "    def __init__(self, dim=768, dropout=DROPOUT_P, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x, need_weights=False)\n",
    "        x = self.norm(x + attn_out)\n",
    "        pooled = x.mean(dim=1)\n",
    "        return self.mlp(pooled)\n",
    "\n",
    "\n",
    "class OCSoftmaxHead(nn.Module):\n",
    "    \"\"\"Regularized one-class style head (trained with BCE).\"\"\"\n",
    "\n",
    "    def __init__(self, dim=768, dropout=DROPOUT_P):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooled = self.norm(x.mean(dim=1))\n",
    "        return self.mlp(pooled)\n",
    "\n",
    "\n",
    "# Initialize classification heads with Multi-GPU support\n",
    "aasist = AASISTHead().to(DEVICE)\n",
    "ocsoft = OCSoftmaxHead().to(DEVICE)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    aasist = nn.DataParallel(aasist)\n",
    "    ocsoft = nn.DataParallel(ocsoft)\n",
    "\n",
    "# Print model summary\n",
    "model_ref = wavlm.module if hasattr(wavlm, \"module\") else wavlm\n",
    "total_params = sum(p.numel() for p in model_ref.parameters())\n",
    "trainable_params = sum(p.numel() for p in aasist.parameters()) + sum(p.numel() for p in ocsoft.parameters())\n",
    "\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"WavLM parameters (frozen): {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Total parameters: {total_params + trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_bh9bBSoiGB"
   },
   "outputs": [],
   "source": [
    "# With oversampling via WeightedRandomSampler, no pos_weight needed.\n",
    "# Label smoothing is applied in the training loop, not in the loss function.\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Collect all trainable parameters (backbone + heads)\n",
    "trainable_params = [p for p in list(wavlm.parameters()) + list(aasist.parameters()) + list(ocsoft.parameters()) if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    trainable_params,\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    " )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=2\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vpz6QIURoj53"
   },
   "outputs": [],
   "source": [
    "def validate(wavlm, aasist, ocsoft, val_loader, criterion, device):\n",
    "    \"\"\"Validation function.\"\"\"\n",
    "    aasist.eval()\n",
    "    ocsoft.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for wavs, labels in tqdm(val_loader, desc=\"Validating\", leave=False, mininterval=1.0):\n",
    "            if wavs is None:\n",
    "                continue\n",
    "\n",
    "            wavs = wavs.float().to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            feats = wavlm(wavs).last_hidden_state\n",
    "\n",
    "            logits_aasist = aasist(feats).squeeze(-1)\n",
    "            logits_oc = ocsoft(feats).squeeze(-1)\n",
    "\n",
    "            loss1 = criterion(logits_aasist, labels.float())\n",
    "            loss2 = criterion(logits_oc, labels.float())\n",
    "            loss = AASIST_WEIGHT * loss1 + OCSOFT_WEIGHT * loss2\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            score_aasist = torch.sigmoid(logits_aasist)\n",
    "            score_oc = torch.sigmoid(logits_oc)\n",
    "            final_score = AASIST_WEIGHT * score_aasist + OCSOFT_WEIGHT * score_oc\n",
    "\n",
    "            all_scores.extend(final_score.detach().cpu().numpy())\n",
    "            all_labels.extend(labels.detach().cpu().numpy())\n",
    "            all_preds.extend((final_score > 0.5).detach().cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / max(len(all_labels) // BATCH_SIZE, 1)  # use actual processed count\n",
    "    accuracy = accuracy_score(all_labels, all_preds) if len(all_labels) else 0.0\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0) if len(all_labels) else 0.0\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0) if len(all_labels) else 0.0\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0) if len(all_labels) else 0.0\n",
    "\n",
    "    labels_np = np.asarray(all_labels)\n",
    "    scores_np = np.asarray(all_scores)\n",
    "    auc = float(\"nan\")\n",
    "    if len(scores_np) and np.isfinite(scores_np).all() and len(np.unique(labels_np)) == 2:\n",
    "        auc = roc_auc_score(labels_np, scores_np)\n",
    "\n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'scores': all_scores,\n",
    "        'labels': all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBhUmxqvokkw",
    "outputId": "9a1156b1-0f73-468a-898b-446c1067c473"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 1330/1330 [48:22<00:00,  2.18s/it, loss=0.0199]\n",
      "Validating: 100%|██████████| 288/288 [15:39<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Train Loss: 0.0360\n",
      "  Val Loss: 0.0091\n",
      "  Val Accuracy: 0.9978\n",
      "  Val Precision: 0.9959\n",
      "  Val Recall: 0.9990\n",
      "  Val F1: 0.9974\n",
      "  Val AUC: 0.9999669375196917\n",
      "  ✓ Best model saved (AUC: 1.0000)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 1330/1330 [29:07<00:00,  1.31s/it, loss=0.0003]\n",
      "Validating: 100%|██████████| 288/288 [04:47<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "  Train Loss: 0.0148\n",
      "  Val Loss: 0.0107\n",
      "  Val Accuracy: 0.9974\n",
      "  Val Precision: 0.9964\n",
      "  Val Recall: 0.9974\n",
      "  Val F1: 0.9969\n",
      "  Val AUC: 0.9999858025819852\n",
      "  Early-stopping counter: 1/3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 1330/1330 [24:27<00:00,  1.10s/it, loss=0.0021]\n",
      "Validating: 100%|██████████| 288/288 [04:56<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "  Train Loss: 0.0126\n",
      "  Val Loss: 0.0083\n",
      "  Val Accuracy: 0.9978\n",
      "  Val Precision: 0.9959\n",
      "  Val Recall: 0.9990\n",
      "  Val F1: 0.9974\n",
      "  Val AUC: 0.9999900812559076\n",
      "  Early-stopping counter: 2/3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 1330/1330 [23:58<00:00,  1.08s/it, loss=0.0010]\n",
      "Validating: 100%|██████████| 288/288 [04:51<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "  Train Loss: 0.0114\n",
      "  Val Loss: 0.0075\n",
      "  Val Accuracy: 0.9980\n",
      "  Val Precision: 0.9964\n",
      "  Val Recall: 0.9990\n",
      "  Val F1: 0.9977\n",
      "  Val AUC: 0.9999963047816126\n",
      "  Early-stopping counter: 3/3\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_accuracy': [],\n",
    "    'val_auc': []\n",
    "}\n",
    "\n",
    "best_auc = 0.0\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training phase\n",
    "    aasist.train()\n",
    "    ocsoft.train()\n",
    "    if UNFREEZE_TOP_LAYERS > 0:\n",
    "        wavlm.train() # Enable dropout/norm in unfrozen layers\n",
    "    else:\n",
    "        wavlm.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    seen_batches = 0\n",
    "    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False, mininterval=1.0)\n",
    "\n",
    "    for step, (wavs, labels) in enumerate(progress):\n",
    "        if wavs is None:\n",
    "            continue\n",
    "\n",
    "        wavs = wavs.float().to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backbone inference: partial gradients if unfrozen\n",
    "        if UNFREEZE_TOP_LAYERS > 0:\n",
    "            output = wavlm(wavs)\n",
    "            feats = output.last_hidden_state\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                output = wavlm(wavs)\n",
    "                feats = output.last_hidden_state\n",
    "\n",
    "        # SpecAugment: random frequency + temporal masking on WavLM features\n",
    "        if SPEC_AUG_FREQ_MASKS > 0 or SPEC_AUG_TIME_MASKS > 0:\n",
    "            feats = feats.clone()\n",
    "            if SPEC_AUG_FREQ_MASKS > 0:\n",
    "                for _ in range(SPEC_AUG_FREQ_MASKS):\n",
    "                    f_start = torch.randint(0, max(feats.size(-1) - SPEC_AUG_FREQ_WIDTH, 1), (1,)).item()\n",
    "                    feats[:, :, f_start:f_start + SPEC_AUG_FREQ_WIDTH] = 0.0\n",
    "            if SPEC_AUG_TIME_MASKS > 0:\n",
    "                for _ in range(SPEC_AUG_TIME_MASKS):\n",
    "                    t_start = torch.randint(0, max(feats.size(1) - SPEC_AUG_TIME_WIDTH, 1), (1,)).item()\n",
    "                    feats[:, t_start:t_start + SPEC_AUG_TIME_WIDTH, :] = 0.0\n",
    "\n",
    "        logits_aasist = aasist(feats).squeeze(-1)\n",
    "        logits_oc = ocsoft(feats).squeeze(-1)\n",
    "\n",
    "        # Label smoothing: prevents loss collapse to 0\n",
    "        smooth_labels = labels.float() * (1.0 - 2.0 * LABEL_SMOOTHING) + LABEL_SMOOTHING\n",
    "\n",
    "        loss1 = criterion(logits_aasist, smooth_labels)\n",
    "        loss2 = criterion(logits_oc, smooth_labels)\n",
    "        loss = AASIST_WEIGHT * loss1 + OCSOFT_WEIGHT * loss2\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(trainable_params, GRAD_CLIP_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        seen_batches += 1\n",
    "        \n",
    "        if step % 20 == 0:\n",
    "            ai_rate = float(labels.float().mean().item())\n",
    "            progress.set_postfix({\n",
    "                'loss': f'{loss.item():.3f}',\n",
    "                'ai%': f'{100.0 * ai_rate:.0f}',\n",
    "                'bs': int(labels.numel()),\n",
    "            })\n",
    "\n",
    "    avg_train_loss = total_loss / max(seen_batches, 1)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    val_metrics = validate(wavlm, aasist, ocsoft, val_loader, criterion, DEVICE)\n",
    "\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "    history['val_auc'].append(val_metrics['auc'])\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f} | Val Loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"  Accuracy: {val_metrics['accuracy']:.4f} | AUC: {val_metrics['auc']}\")\n",
    "\n",
    "    metric_for_scheduler = val_metrics['auc'] if np.isfinite(val_metrics['auc']) else val_metrics['accuracy']\n",
    "    scheduler.step(metric_for_scheduler)\n",
    "\n",
    "    # Save checkpoint\n",
    "    ckpt = {\n",
    "        'wavlm': wavlm.state_dict(),\n",
    "        'aasist': aasist.state_dict(),\n",
    "        'ocsoft': ocsoft.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'epoch': epoch + 1,\n",
    "        'val_metrics': val_metrics,\n",
    "        'history': history,\n",
    "        'config': { 'target_duration': TARGET_DURATION, 'sample_rate': SAMPLE_RATE },\n",
    "    }\n",
    "\n",
    "    torch.save(ckpt, f\"{SAVE_DIR}/latest_model.pt\")\n",
    "    if epoch == 0: torch.save(ckpt, f\"{SAVE_DIR}/epoch_1.pt\")\n",
    "\n",
    "    current_auc = val_metrics['auc']\n",
    "    if np.isfinite(current_auc) and current_auc > best_auc + MIN_DELTA_AUC:\n",
    "        best_auc = current_auc\n",
    "        no_improve = 0\n",
    "        torch.save(ckpt, f\"{SAVE_DIR}/best_model.pt\")\n",
    "        print(f\"  ✓ Save Best Model (AUC: {best_auc:.4f})\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"  Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(\"-\" * 30)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04c91dcabde547dd8faa3759b73c74a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0651902164774e4abbeee973acc6e866",
      "placeholder": "​",
      "style": "IPY_MODEL_ef50a0921ba54abfa65ef56cc40524cb",
      "value": "config.json: "
     }
    },
    "062b30d1aa4647df8d3cd9230176888f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0651902164774e4abbeee973acc6e866": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11bc14fabe0e45728b72248b1aa57ce3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23722f8d539e470392cd349e28d04d27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b85d9c475da4622ba9294c391172257",
      "max": 377617425,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ed265f84007f425fb71e80cee00aceb6",
      "value": 377617425
     }
    },
    "33997416e5a749a5a9f2318e58c72396": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_04c91dcabde547dd8faa3759b73c74a4",
       "IPY_MODEL_dc1eb4ac7fcd4990bf9ca03da42b083c",
       "IPY_MODEL_ab0b2b6c45e74d088df8c3bfb8464ba4"
      ],
      "layout": "IPY_MODEL_f5c9f78837974eb483017fbc01b37f65"
     }
    },
    "43c7a4523d4542f0b7c1d8073d71e03c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f1478823fda4d43a38897e88a4acbc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a2a3a7cbf3ab4c8e9aa67c242f5ee554",
       "IPY_MODEL_23722f8d539e470392cd349e28d04d27",
       "IPY_MODEL_991d5b5f79b942cba4dd256c38b0698f"
      ],
      "layout": "IPY_MODEL_bb665bf6e6854bddb66ef360a87043a9"
     }
    },
    "67c47acf52094558b13c3d2582a480cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "991d5b5f79b942cba4dd256c38b0698f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11bc14fabe0e45728b72248b1aa57ce3",
      "placeholder": "​",
      "style": "IPY_MODEL_a52d1cb9694547c3a72576bf755843d4",
      "value": " 378M/378M [00:02&lt;00:00, 190MB/s]"
     }
    },
    "9b85d9c475da4622ba9294c391172257": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2a3a7cbf3ab4c8e9aa67c242f5ee554": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_062b30d1aa4647df8d3cd9230176888f",
      "placeholder": "​",
      "style": "IPY_MODEL_b08048eb081745eb847a45697c22fb6c",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "a52d1cb9694547c3a72576bf755843d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab0b2b6c45e74d088df8c3bfb8464ba4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43c7a4523d4542f0b7c1d8073d71e03c",
      "placeholder": "​",
      "style": "IPY_MODEL_67c47acf52094558b13c3d2582a480cb",
      "value": " 2.24k/? [00:00&lt;00:00, 215kB/s]"
     }
    },
    "b08048eb081745eb847a45697c22fb6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb665bf6e6854bddb66ef360a87043a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb692922692f4539990d7f4833edaf0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "dc1eb4ac7fcd4990bf9ca03da42b083c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb692922692f4539990d7f4833edaf0c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fbcd1f96f57b427fa91d2bab8e6c311f",
      "value": 1
     }
    },
    "ed265f84007f425fb71e80cee00aceb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ef50a0921ba54abfa65ef56cc40524cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5c9f78837974eb483017fbc01b37f65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbcd1f96f57b427fa91d2bab8e6c311f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
